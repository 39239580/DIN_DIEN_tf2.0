{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from layer import attention, dice, AUGRU\n",
    "from utils import sequence_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(tf.keras.Model):\n",
    "    #\n",
    "    def __init__(self, user_count, item_count, cate_count, cate_list,\n",
    "                       user_dim, item_dim, cate_dim,\n",
    "                       dim_layers):\n",
    "        \"\"\"\n",
    "        user_count: user 数量\n",
    "        item_count：商品数量\n",
    "        cate_count：商品类别数量\n",
    "        cate_list：商品类别列表\n",
    "        user_dim，item_dim，cate_dim：用户，商品，商品类别的嵌入维度\n",
    "        \"\"\"\n",
    "        super(Base, self).__init__()\n",
    "        self.item_dim = item_dim\n",
    "        self.cate_dim = cate_dim\n",
    "        \n",
    "        # 执行嵌入操作\n",
    "        self.user_emb = nn.Embedding(user_count, user_dim)\n",
    "        self.item_emb = nn.Embedding(item_count, item_dim)\n",
    "        self.cate_emb = nn.Embedding(cate_count, cate_dim)\n",
    "        \n",
    "        # 初始化变量\n",
    "        self.item_bias= tf.Variable(tf.zeros([item_count]), trainable=True)\n",
    "        self.cate_list = cate_list\n",
    "        \n",
    "        # 初始化一些工具层\n",
    "        self.hist_bn = nn.BatchNormalization()\n",
    "        self.hist_fc = nn.Dense(item_dim+cate_dim)\n",
    "        \n",
    "        # 初始化一些全连接层，将embedding的结果直接传入全连接层。直接调用self.fc(x)即可\n",
    "        self.fc = tf.keras.Sequential()\n",
    "        self.fc.add(nn.BatchNormalization())\n",
    "        for dim_layer in dim_layers[:-1]:\n",
    "            self.fc.add(nn.Dense(dim_layer, activation='sigmoid'))\n",
    "        self.fc.add(nn.Dense(dim_layers[-1], activation=None))\n",
    "\n",
    "    # 生成各种embedding\n",
    "    def get_emb(self, user, item, history):\n",
    "        # 假设每个user的history序列长度均为 len_his\n",
    "        # 根据user=[2,1,4]获取对应的嵌入结果。\n",
    "        user_emb = self.user_emb(user)  # (None,user_dim)\n",
    "        \n",
    "        # 根据 item=[3,4,5]获取对应的嵌入结果。\n",
    "        item_emb = self.item_emb(item)    # (None,item_dim)\n",
    "        \n",
    "        # tf.gather() 根据索引，获取item对应的类别，并得到其embedding结果\n",
    "        item_cate_emb = self.cate_emb(tf.gather(self.cate_list, item)) # (None,cate_dim)\n",
    "        item_join_emb = tf.concat([item_emb, item_cate_emb], -1)  # 合并商品和类别embedding (None, item_dim + cate_dim)\n",
    "        \n",
    "        # 获取item 对应的item_bias\n",
    "        item_bias= tf.gather(self.item_bias, item)\n",
    "    \n",
    "        # 取出history(np.array格式)中的所有的item_id 对应的embedding \n",
    "        hist_emb = self.item_emb(history)     #  (None,len_his,item_dim)\n",
    "        \n",
    "         # tf.gather() 根据索引，获取 history中的item 对应的类别，并得到其embedding结果\n",
    "        hist_cate_emb = self.cate_emb(tf.gather(self.cate_list, history))    #  (None,len_his,cate_dim)\n",
    "        hist_join_emb = tf.concat([hist_emb, hist_cate_emb], -1)    # (None,len_his,cate_dim+item_dim)\n",
    "\n",
    "        return user_emb, item_join_emb, item_bias, hist_join_emb\n",
    "\n",
    "    def call(self, user, item, history, length):\n",
    "        \"\"\"\n",
    "        user, item: 用户，商品id\n",
    "        history: 用户的交互序列\n",
    "        length: [4.7,9,6,....,]一共batch_size个，记录用户history的有效长度(history个数)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 通过传入参数，获取某一个batch的embedding 结果\n",
    "        user_emb, item_join_emb, item_bias, hist_join_emb = self.get_emb(user, item, history)\n",
    "\n",
    "        # 因为每个user的history有效长度不一样，因此我们需要通过mask机制，取出有效部分\n",
    "        # 为了方便理解，假定max(length)=20,即history的序列长度也为20\n",
    "        # length[0]=4,所以tf.sequence_mask对第一个user 生成[1,1,1,1,0,0...0]（共20个元素）\n",
    "        hist_mask = tf.sequence_mask(length, max(length), dtype=tf.float32)     # (None,20)\n",
    "        # tf.tile对指定维度进行复制\n",
    "        hist_mask = tf.tile(tf.expand_dims(hist_mask, -1), (1,1,self.item_dim+self.cate_dim))  # (None,20,item_dim+cate_dim)\n",
    "        \n",
    "        # 通过 tf.math.multiply 将无效的部分掩盖（与0相乘）\n",
    "        hist_join_emb = tf.math.multiply(hist_join_emb, hist_mask) # (None,20,item_dim+cate_dim)\n",
    "        \n",
    "        # 以上完成嵌入，以下(1)(2)进行sumpooling\n",
    "        #（1） sum\n",
    "        hist_join_emb = tf.reduce_sum(hist_join_emb, 1) # (None,item_dim+cate_dim)\n",
    "        \n",
    "        # （2）求均值，将用户求和的结果/对应的序列长度\n",
    "        hist_join_emb = tf.math.divide(hist_join_emb, tf.cast(tf.tile(tf.expand_dims(length, -1),\n",
    "                                                      [1,self.item_dim+self.cate_dim]), tf.float32))   # (None,item_dim + cate_dim)\n",
    "\n",
    "        # 将sumpooling的结果BatchNormalization，然后丢入全连接层，得到最终的hist_hid_emb：shape = # (None,item_dim+cate_dim)\n",
    "        hist_hid_emb = self.hist_fc(self.hist_bn(hist_join_emb))\n",
    "        \n",
    "        # 将所有的embedding结果concat得到 join_emb\n",
    "        join_emb = tf.concat([user_emb, item_join_emb, hist_hid_emb], -1) # (None,user_dim+ item_dim+cate_dim+ item_dim+cate_dim)\n",
    "\n",
    "        # 将join_emb传入全连接层\n",
    "        output = tf.squeeze(self.fc(join_emb)) + item_bias\n",
    "        logit = tf.keras.activations.sigmoid(output)\n",
    "\n",
    "        return output, logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关键在于attention的实现\n",
    "class attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, keys_dim, dim_layers):\n",
    "        super(attention, self).__init__()\n",
    "        self.keys_dim = keys_dim\n",
    "\n",
    "        self.fc = tf.keras.Sequential()\n",
    "        for dim_layer in dim_layers[:-1]:\n",
    "            self.fc.add(nn.Dense(dim_layer, activation='sigmoid'))\n",
    "        self.fc.add(nn.Dense(dim_layers[-1], activation=None))\n",
    "\n",
    "    def call(self, queries, keys, keys_length):\n",
    "        queries = tf.tile(tf.expand_dims(queries, 1), [1, tf.shape(keys)[1], 1])\n",
    "        # outer product ?\n",
    "        din_all = tf.concat([queries, keys, queries-keys, queries*keys], axis=-1)\n",
    "        outputs = tf.transpose(self.fc(din_all), [0,2,1])\n",
    "\n",
    "        # Mask\n",
    "        key_masks = tf.sequence_mask(keys_length, max(keys_length), dtype=tf.bool)  # [B, T]\n",
    "        key_masks = tf.expand_dims(key_masks, 1)\n",
    "        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
    "        outputs = tf.where(key_masks, outputs, paddings)  # [B, 1, T]\n",
    "\n",
    "        # Scale\n",
    "        outputs = outputs / (self.keys_dim ** 0.5)\n",
    "\n",
    "        # Activation\n",
    "        outputs = tf.keras.activations.softmax(outputs, -1)  # [B, 1, T]\n",
    "\n",
    "        # Weighted sum\n",
    "        outputs = tf.squeeze(tf.matmul(outputs, keys))  # [B, H]\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIN(Base):\n",
    "    def __init__(self, user_count, item_count, cate_count, cate_list,\n",
    "                       user_dim, item_dim, cate_dim,\n",
    "                       dim_layers):\n",
    "        super(DIN, self).__init__(user_count, item_count, cate_count, cate_list,\n",
    "                                  user_dim, item_dim, cate_dim,\n",
    "                                  dim_layers)\n",
    "\n",
    "        self.hist_at = attention(item_dim+cate_dim, dim_layers)\n",
    "\n",
    "        self.fc = tf.keras.Sequential()\n",
    "        self.fc.add(nn.BatchNormalization())\n",
    "        for dim_layer in dim_layers[:-1]:\n",
    "            self.fc.add(nn.Dense(dim_layer, activation=None))\n",
    "            self.fc.add(dice(dim_layer))\n",
    "        self.fc.add(nn.Dense(dim_layers[-1], activation=None))\n",
    "\n",
    "    def call(self, user, item, history, length):\n",
    "        user_emb, item_join_emb, item_bias, hist_join_emb = self.get_emb(user, item, history)\n",
    "\n",
    "        hist_attn_emb = self.hist_at(item_join_emb, hist_join_emb, length)\n",
    "        hist_attn_emb = self.hist_fc(self.hist_bn(hist_attn_emb))\n",
    "\n",
    "        join_emb = tf.concat([user_emb, item_join_emb, hist_attn_emb], -1)\n",
    "\n",
    "        output = tf.squeeze(self.fc(join_emb)) + item_bias\n",
    "        logit = tf.keras.activations.sigmoid(output)\n",
    "\n",
    "        return output, logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
